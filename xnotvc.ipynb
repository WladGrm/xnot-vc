{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from wavlm.WavLM import WavLM, WavLMConfig\n",
    "from hifigan.models import Generator as HiFiGAN\n",
    "from hifigan.utils import AttrDict\n",
    "from matcher import ExNOTVC\n",
    "import torchaudio\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "DEVICE = 'xpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xnot_vc(pretrained=True, progress=True, prematched=False, device='xpu') -> ExNOTVC:\n",
    "    \"\"\" Load kNN-VC (WavLM encoder and HiFiGAN decoder). Optionally use vocoder trained on `prematched` data. \"\"\"\n",
    "    hifigan, hifigan_cfg = hifigan_wavlm(pretrained, progress, prematched, device)\n",
    "    wavlm = wavlm_large(pretrained, progress, device)\n",
    "    xnotvc = ExNOTVC(wavlm, hifigan, hifigan_cfg, device)\n",
    "    return xnotvc\n",
    "\n",
    "\n",
    "def hifigan_wavlm(pretrained=True, progress=True, prematched=False, device='xpu') -> HiFiGAN:\n",
    "    \"\"\" Load pretrained hifigan trained to vocode wavlm features. Optionally use weights trained on `prematched` data. \"\"\"\n",
    "    #cp = Path(__file__).parent.absolute()\n",
    "\n",
    "    with open('hifigan/config_v1_wavlm.json') as f:\n",
    "        data = f.read()\n",
    "    json_config = json.loads(data)\n",
    "    h = AttrDict(json_config)\n",
    "    device = torch.device(device)\n",
    "\n",
    "    generator = HiFiGAN(h).to(device)\n",
    "    \n",
    "    if pretrained:\n",
    "        if prematched:\n",
    "            url = \"https://github.com/bshall/knn-vc/releases/download/v0.1/prematch_g_02500000.pt\"\n",
    "        else:\n",
    "            print(\"Загружаем непреметченный\")\n",
    "            url = \"https://github.com/bshall/knn-vc/releases/download/v0.1/g_02500000.pt\"\n",
    "        state_dict_g = torch.hub.load_state_dict_from_url(\n",
    "            url,\n",
    "            map_location=device,\n",
    "            progress=progress\n",
    "        )\n",
    "        generator.load_state_dict(state_dict_g['generator'])\n",
    "    generator.eval()\n",
    "    generator.remove_weight_norm()\n",
    "    print(f\"[HiFiGAN] Generator loaded with {sum([p.numel() for p in generator.parameters()]):,d} parameters.\")\n",
    "    return generator, h\n",
    "\n",
    "\n",
    "def wavlm_large(pretrained=True, progress=True, device='xpu') -> WavLM:\n",
    "    \"\"\"Load the WavLM large checkpoint from the original paper. See https://github.com/microsoft/unilm/tree/master/wavlm for details. \"\"\"\n",
    "    if torch.xpu.is_available() is False:\n",
    "        if str(device) != 'cpu':\n",
    "            logging.warning(f\"Overriding device {device} to cpu since no GPU is available.\")\n",
    "            device = 'cpu'\n",
    "    checkpoint = torch.hub.load_state_dict_from_url(\n",
    "        \"https://github.com/bshall/knn-vc/releases/download/v0.1/WavLM-Large.pt\", \n",
    "        map_location=device, \n",
    "        progress=progress\n",
    "    )\n",
    "    \n",
    "    cfg = WavLMConfig(checkpoint['cfg'])\n",
    "    device = torch.device(device)\n",
    "    model = WavLM(cfg)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"WavLM-Large loaded with {sum([p.numel() for p in model.parameters()]):,d} parameters.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serjio/mambaforge/envs/pytorch-arc/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем непреметченный\n",
      "Removing weight norm...\n",
      "[HiFiGAN] Generator loaded with 16,523,393 parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 13:32:50,627 - wavlm.WavLM - INFO - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WavLM-Large loaded with 315,453,120 parameters.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xnotvc = xnot_vc() #загружаем веса для WavLM, HiFi Gan, убираем флажок преметчинга \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пути к аудиодорожкам с женским голосом и мужским"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain source - домен с исходным голосом, Domain target - домен с целевым голосом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "domain_target_path = \"experiments/2300/to_female/female_3570_30.flac\"\n",
    "domain_source_path = \"experiments/2300/male_2300_30.flac\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодируем аудиодорожки с помощью WavLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_source = xnotvc.get_features(domain_source_path, )\n",
    "features_target = xnotvc.get_features(domain_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **KNNVC TEST** (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_path = \"experiments/2300/valid_long.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_wav_paths = [domain_target_path] #target\n",
    "src_wav_path = validation_path  #validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_seq = xnotvc.get_features(src_wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_set = xnotvc.get_matching_set(ref_wav_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([880, 1024])\n"
     ]
    }
   ],
   "source": [
    "out_wav = xnotvc.match(query_seq, matching_set, topk=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torchaudio.save('trash/2300/res.flac', out_wav[None], 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **END OF TEST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WavLM каждые ~20-30 мс звуковой дорожки представляет в виде вектора фичей размерностью 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1493, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(features_source.shape) #[длина дорожки делить на ~20-30мс , 1024] = массив из 7971 векторов размерностью 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1418, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(features_target.shape) #массив из 5473 векторов размерностью 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler from voice distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайным образом выбираем батч векторов фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1418, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(features_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_from_tensor(t, batch_size = 32):\n",
    "    tensor = t.detach().cpu().numpy()\n",
    "    batch = np.empty((batch_size, 1024))\n",
    "    for i in range(batch_size):\n",
    "        indice = random.randrange(0, tensor.shape[0])\n",
    "        sample_tensor = tensor[indice]\n",
    "        batch[i]=sample_tensor\n",
    "    return batch.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1024)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_from_tensor(features_source).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1024)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_from_tensor(features_target).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция стоимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sq_cost(X,Y):\n",
    "    return (X-Y).square().flatten(start_dim=1).mean(dim=1)\n",
    "\n",
    "COST = sq_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NNs for transport map ***T*** : R^1024 -> R^1024 (generator) and potential ***f*** : R^1024 -> R (discriminator).\n",
    "Medium size multilayer perceptrons with ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T params: 8396800\n",
      "f params: 7348225\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "class NegAbs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NegAbs, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return -torch.abs(input)\n",
    "\n",
    "T = nn.Sequential(\n",
    "    *itertools.chain.from_iterable([[nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),] for _ in range(7)]),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    ").to(DEVICE)\n",
    "\n",
    "f = nn.Sequential(\n",
    "    *itertools.chain.from_iterable([[nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),] for _ in range(7)]),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    # nn.Linear(1024, 1024),\n",
    "    # nn.ReLU(True),\n",
    "    nn.Linear(1024, 1),\n",
    "    NegAbs(),\n",
    ").to(DEVICE)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "\n",
    "print('T params:', np.sum([np.prod(p.shape) for p in T.parameters()]))\n",
    "print('f params:', np.sum([np.prod(p.shape) for p in f.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "T_opt = torch.optim.Adam(T.parameters(), lr=1e-4, weight_decay=1e-10)\n",
    "f_opt = torch.optim.Adam(f.parameters(), lr=1e-4, weight_decay=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "T_ITERS = 10 # T updates per 1 f update\n",
    "MAX_ITERS = 20001\n",
    "\n",
    "to_torch = lambda x: torch.Tensor(x).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = to_torch(sample_from_tensor(features_source))\n",
    "Y = to_torch(sample_from_tensor(features_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16.7152, 21.0695, 10.0319, 16.4119, 19.3818, 18.5900, 17.9818, 12.2870,\n",
       "        15.0837,  9.7668, 17.7889, 21.6511, 19.4071, 13.0706, 21.2476, 12.2096,\n",
       "        12.4429, 14.6963, 20.2154, 17.1835, 17.8451,  9.5174, 14.4012, 17.2628,\n",
       "        16.7337, 17.0727,  9.0887, 17.7961, 15.4787, 17.3981, 16.1379, 14.5723],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_cost(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сбрасываем веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_reset(T); weight_reset(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр W из алгоритма неполного транспорта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм неполного оптимального транспорта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20000\n",
      "T_loss [np.float32(3.4607031), np.float32(3.1610453), np.float32(3.1301904), np.float32(2.980997), np.float32(2.8818643), np.float32(2.8462367), np.float32(3.0384846), np.float32(3.2547486), np.float32(3.2168407), np.float32(3.273753)]\n",
      "f_loss [array(-0.04195407, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#XNOT algo\n",
    "\n",
    "for step in tqdm (range(MAX_ITERS)):\n",
    "    T_loss_ar = []\n",
    "    f_loss_ar = []\n",
    "    #T optimization\n",
    "    T.train(True); f.eval()\n",
    "    for t_iter in range(T_ITERS):\n",
    "        X = to_torch(sample_from_tensor(features_source)) \n",
    "        T_loss = COST(X, T(X)).mean() - f(T(X)).mean()\n",
    "        with torch.no_grad():\n",
    "            T_loss_ar.append(T_loss.cpu().detach().numpy().mean())\n",
    "        T_opt.zero_grad(); T_loss.backward(); T_opt.step()\n",
    "\n",
    "    #f optimization\n",
    "    T.eval(); f.train(True)\n",
    "    X, Y = to_torch(sample_from_tensor(features_source)), to_torch(sample_from_tensor(features_target))\n",
    "    f_loss = f(T(X)).mean() - (W * f(Y)).mean()\n",
    "    with torch.no_grad():\n",
    "            f_loss_ar.append(f_loss.cpu().detach().numpy())\n",
    "    f_opt.zero_grad(); f_loss.backward(); f_opt.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Step\", step)\n",
    "        print(\"T_loss\", T_loss_ar)\n",
    "        print(\"f_loss\", f_loss_ar)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(T.state_dict(), \"checkpoints/xnot_vc_2300_3570_30s_20k_W_20.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Транспорт в новый домен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция, которая заменяет каждый исходный вектор фичей на преобразованный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xnot_transform(source_tensor):\n",
    "    with torch.no_grad():\n",
    "        result = torch.empty(source_tensor.shape[0],1024) #Initialize empty tensor with the same dimensions as an input feature vector\n",
    "        for i in range(source_tensor.shape[0]):\n",
    "            result[i] = T(source_tensor[i])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем звуковую дорожку с женским голосом, которой не было в тренировочной выборке и формируем массив из векторов фичей с помощью WavLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_path = \"experiments/2300/valid_long.flac\" #Путь до файла с женским голосом для валидации\n",
    "features_valid = xnotvc.get_features(validation_path) #Формируем массив из векторов фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([880, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(features_valid.shape) #Исходный массив: 646 векторов размерностью 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем преобразованный массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feautures_transformed = xnot_transform(features_valid) #Преобразованный массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([880, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(feautures_transformed.shape) #Размерность преобразованного массива должна совпасть с размерностью исходного = 646х1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5977,  1.7348, -2.3709,  ...,  0.5059, -3.1327, -1.8742])\n"
     ]
    }
   ],
   "source": [
    "print(feautures_transformed[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вокодинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем метод для вокодинга фичей с помощью HiFi GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_wav = xnotvc.vocode(feautures_transformed[None].to(DEVICE)).cpu().squeeze() #Вокодинг преобразованного с помощью XNOT вектора фичей\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([281600])\n"
     ]
    }
   ],
   "source": [
    "print(out_wav.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем файл с аудиодорожкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save('xnotvc_2300_3570_30_long_W20.flac', out_wav[None], 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
