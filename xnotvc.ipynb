{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from wavlm.WavLM import WavLM, WavLMConfig\n",
    "from hifigan.models import Generator as HiFiGAN\n",
    "from hifigan.utils import AttrDict\n",
    "from matcher import ExNOTVC\n",
    "import torchaudio\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xnot_vc(pretrained=True, progress=True, prematched=False, device='cuda') -> ExNOTVC:\n",
    "    \"\"\" Load kNN-VC (WavLM encoder and HiFiGAN decoder). Optionally use vocoder trained on `prematched` data. \"\"\"\n",
    "    hifigan, hifigan_cfg = hifigan_wavlm(pretrained, progress, prematched, device)\n",
    "    wavlm = wavlm_large(pretrained, progress, device)\n",
    "    xnotvc = ExNOTVC(wavlm, hifigan, hifigan_cfg, device)\n",
    "    return xnotvc\n",
    "\n",
    "\n",
    "def hifigan_wavlm(pretrained=True, progress=True, prematched=False, device='cuda') -> HiFiGAN:\n",
    "    \"\"\" Load pretrained hifigan trained to vocode wavlm features. Optionally use weights trained on `prematched` data. \"\"\"\n",
    "    #cp = Path(__file__).parent.absolute()\n",
    "\n",
    "    with open('hifigan/config_v1_wavlm.json') as f:\n",
    "        data = f.read()\n",
    "    json_config = json.loads(data)\n",
    "    h = AttrDict(json_config)\n",
    "    device = torch.device(device)\n",
    "\n",
    "    generator = HiFiGAN(h).to(device)\n",
    "    \n",
    "    if pretrained:\n",
    "        if prematched:\n",
    "            url = \"https://github.com/bshall/knn-vc/releases/download/v0.1/prematch_g_02500000.pt\"\n",
    "        else:\n",
    "            print(\"Загружаем непреметченный\")\n",
    "            url = \"https://github.com/bshall/knn-vc/releases/download/v0.1/g_02500000.pt\"\n",
    "        state_dict_g = torch.hub.load_state_dict_from_url(\n",
    "            url,\n",
    "            map_location=device,\n",
    "            progress=progress\n",
    "        )\n",
    "        generator.load_state_dict(state_dict_g['generator'])\n",
    "    generator.eval()\n",
    "    generator.remove_weight_norm()\n",
    "    print(f\"[HiFiGAN] Generator loaded with {sum([p.numel() for p in generator.parameters()]):,d} parameters.\")\n",
    "    return generator, h\n",
    "\n",
    "\n",
    "def wavlm_large(pretrained=True, progress=True, device='cuda') -> WavLM:\n",
    "    \"\"\"Load the WavLM large checkpoint from the original paper. See https://github.com/microsoft/unilm/tree/master/wavlm for details. \"\"\"\n",
    "    if torch.cuda.is_available() == False:\n",
    "        if str(device) != 'cpu':\n",
    "            logging.warning(f\"Overriding device {device} to cpu since no GPU is available.\")\n",
    "            device = 'cpu'\n",
    "    checkpoint = torch.hub.load_state_dict_from_url(\n",
    "        \"https://github.com/bshall/knn-vc/releases/download/v0.1/WavLM-Large.pt\", \n",
    "        map_location=device, \n",
    "        progress=progress\n",
    "    )\n",
    "    \n",
    "    cfg = WavLMConfig(checkpoint['cfg'])\n",
    "    device = torch.device(device)\n",
    "    model = WavLM(cfg)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"WavLM-Large loaded with {sum([p.numel() for p in model.parameters()]):,d} parameters.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wlad/projects/smiles/xnot-vc/.venv/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем непреметченный\n",
      "Removing weight norm...\n",
      "[HiFiGAN] Generator loaded with 16,523,393 parameters.\n",
      "WavLM-Large loaded with 315,453,120 parameters.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xnotvc = xnot_vc() #загружаем веса для WavLM, HiFi Gan, убираем флажок преметчинга \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пути к аудиодорожкам с женским голосом и мужским"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain female - домен с женским голосом, Domain male - домен с мужским голосом. Мы хотим из женского голоса сделать мужской"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_female_path = \"data/female_3112.flac\"\n",
    "domain_male_path = \"data/male.flac\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодируем аудиодорожки с помощью WavLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_female = xnotvc.get_features(domain_female_path)\n",
    "features_male = xnotvc.get_features(domain_male_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WavLM каждые ~20-30 мс звуковой дорожки представляет в виде вектора фичей размерностью 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7971, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(features_female.shape) #[длина дорожки делить на ~20-30мс , 1024] = массив из 7971 векторов размерностью 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5473, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(features_male.shape) #массив из 5473 векторов размерностью 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler from voice distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайным образом выбираем батч векторов фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7971, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(features_female.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_tensor(t, batch_size = 64):\n",
    "    tensor = t.detach().cpu().numpy()\n",
    "    batch = np.empty((batch_size, 1024))\n",
    "    for i in range(batch_size):\n",
    "        indice = random.randrange(0, tensor.shape[0])\n",
    "        sample_tensor = tensor[indice]\n",
    "        batch[i]=sample_tensor\n",
    "    return batch.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_from_tensor(features_female).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1024)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_from_tensor(features_male).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция стоимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq_cost(X,Y):\n",
    "    return (X-Y).square().flatten(start_dim=1).mean(dim=1)\n",
    "\n",
    "COST = sq_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NNs for transport map ***T*** : R^1024 -> R^1024 (generator) and potential ***f*** : R^1024 -> R (discriminator).\n",
    "Medium size multilayer perceptrons with ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T params: 8396800\n",
      "f params: 7348225\n"
     ]
    }
   ],
   "source": [
    "class NegAbs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NegAbs, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return -torch.abs(input)\n",
    "\n",
    "T = nn.Sequential(\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024)\n",
    ").to(DEVICE)\n",
    "\n",
    "f = nn.Sequential(\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1),\n",
    "    NegAbs(),\n",
    ").to(DEVICE)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "\n",
    "print('T params:', np.sum([np.prod(p.shape) for p in T.parameters()]))\n",
    "print('f params:', np.sum([np.prod(p.shape) for p in f.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T_opt = torch.optim.Adam(T.parameters(), lr=1e-4, weight_decay=1e-10)\n",
    "f_opt = torch.optim.Adam(f.parameters(), lr=1e-4, weight_decay=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "T_ITERS = 10 # T updates per 1 f update\n",
    "MAX_ITERS = 20001\n",
    "\n",
    "to_torch = lambda x: torch.Tensor(x).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = to_torch(sample_from_tensor(features_female))\n",
    "Y = to_torch(sample_from_tensor(features_male))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.6744, 15.9143, 13.5941, 16.7626, 14.5010, 17.5511, 11.6979, 14.8074,\n",
       "        12.4722, 14.6000, 17.1989, 15.4499, 16.9147, 16.2463, 14.0737, 14.6721,\n",
       "        12.9823, 17.0361, 15.8620, 17.7364, 12.0871, 15.6339, 14.6636, 17.5646,\n",
       "        13.5528,  9.6802, 16.2999, 17.0646, 11.2215, 13.2988, 13.6073, 15.4935,\n",
       "        11.8002, 13.5617, 17.8838, 16.6961, 17.9615, 16.9645, 14.9347, 17.2053,\n",
       "        13.3959, 15.2365,  9.7649, 11.7503, 16.7646, 14.0633, 14.7524, 11.3784,\n",
       "        14.3227, 12.3387, 14.6011, 13.4172, 15.9979, 13.4735, 14.1175, 12.2714,\n",
       "        12.5115,  5.5139, 15.7411, 16.2136, 12.5414, 15.1741, 14.8726, 13.9692],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_cost(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сбрасываем веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_reset(T); weight_reset(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр W из алгоритма неполного транспорта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм неполного оптимального транспорта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2100\n",
      "T_loss [np.float32(2.5054152), np.float32(2.8232763), np.float32(2.6821666), np.float32(2.8660436), np.float32(2.7107248), np.float32(2.6773112), np.float32(2.63901), np.float32(2.6298006), np.float32(2.6032062), np.float32(2.6912763)]\n",
      "f_loss [array(0.25828195, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#XNOT algo\n",
    "\n",
    "for step in tqdm (range(MAX_ITERS)):\n",
    "    T_loss_ar = []\n",
    "    f_loss_ar = []\n",
    "    #T optimization\n",
    "    T.train(True); f.eval()\n",
    "    for t_iter in range(T_ITERS):\n",
    "        X = to_torch(sample_from_tensor(features_female)) \n",
    "        T_loss = COST(X, T(X)).mean() - f(T(X)).mean()\n",
    "        with torch.no_grad():\n",
    "            T_loss_ar.append(T_loss.cpu().detach().numpy().mean())\n",
    "        T_opt.zero_grad(); T_loss.backward(); T_opt.step()\n",
    "\n",
    "    #f optimization\n",
    "    T.eval(); f.train(True)\n",
    "    X, Y = to_torch(sample_from_tensor(features_female)), to_torch(sample_from_tensor(features_male))\n",
    "    f_loss = f(T(X)).mean() - (W * f(Y)).mean()\n",
    "    with torch.no_grad():\n",
    "            f_loss_ar.append(f_loss.cpu().detach().numpy())\n",
    "    f_opt.zero_grad(); f_loss.backward(); f_opt.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Step\", step)\n",
    "        print(\"T_loss\", T_loss_ar)\n",
    "        print(\"f_loss\", f_loss_ar)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Транспорт в новый домен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция, которая заменяет каждый исходный вектор фичей на преобразованный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xnot_transform(source_tensor):\n",
    "    with torch.no_grad():\n",
    "        result = torch.empty(source_tensor.shape[0],1024) #Initialize empty tensor with the same dimensions as an input feature vector\n",
    "        for i in range(source_tensor.shape[0]):\n",
    "            result[i] = T(source_tensor[i])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем звуковую дорожку с женским голосом, которой не было в тренировочной выборке и формируем массив из векторов фичей с помощью WavLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_path = \"data/valid.flac\" #Путь до файла с женским голосом для валидации\n",
    "features_valid = xnotvc.get_features(validation_path) #Формируем массив из векторов фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_valid.shape) #Исходный массив: 646 векторов размерностью 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем преобразованный массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feautures_transformed = xnot_transform(features_valid) #Преобразованный массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feautures_transformed.shape) #Размерность преобразованного массива должна совпасть с размерностью исходного = 646х1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feautures_transformed[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вокодинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем метод для вокодинга фичей с помощью HiFi GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_wav = xnotvc.vocode(feautures_transformed[None].to(DEVICE)).cpu().squeeze() #Вокодинг преобразованного с помощью XNOT вектора фичей\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_wav.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеч. У меня внутри ноутбука не выводит звук даже на оригинальных примерах от авторов knn-vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(out_wav.numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем файл с аудиодорожкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save('xnot_vc_test.flac', out_wav[None], 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка на обратный вокодинг неизмененного вектора фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_wav_test = xnotvc.vocode(features_valid[None].to(DEVICE)).cpu().squeeze() #Вокодинг вектора фичей, полученного с помощью WavLM -> должны получить исходную аудиодорожку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_wav_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save('data/hifi_gan_test.flac', out_wav_test[None], 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Вывод:*** Hifi GAN преобразовал фичи исходной последовательности и восстановил звуковую дорожку"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "not",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
